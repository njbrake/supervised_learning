{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a26976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "from functools import lru_cache\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b896d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def load_heart():\n",
    "    # https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "    df = pd.read_csv(\"data/heart.csv\")\n",
    "    X = df.iloc[:,0:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f78003",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def load_mobile():\n",
    "    # https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "    df = pd.read_csv(\"data/phone_price.csv\")\n",
    "    X = df.iloc[:,0:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706937e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def load_adult():\n",
    "    df = pd.read_csv(\"data/adult.csv\")\n",
    "    df = pd.get_dummies(df)\n",
    "    df = df.drop(df.columns[-2], axis=1)\n",
    "    X = df.iloc[:,0:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1338026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def load_weather():\n",
    "    # https://www.kaggle.com/jsphyg/weather-dataset-rattle-package?select=weatherAUS.csv\n",
    "    df = pd.read_csv(\"data/weather.csv\")\n",
    "    df = df.dropna(axis=0)\n",
    "    df = pd.get_dummies(df, dummy_na=False)\n",
    "    # drop the nan column\n",
    "    df = df.drop(df.columns[-1], axis=1)\n",
    "    # drop the no column\n",
    "    df = df.drop(df.columns[-2], axis=1)\n",
    "    X = df.iloc[:,0:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0a10a",
   "metadata": {},
   "source": [
    "# Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1092d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figs(dX, y, name):\n",
    "    plt.matshow(X.corr())\n",
    "    plt.yticks(np.arange(X.shape[1]), X.columns)\n",
    "    plt.xticks(np.arange(X.shape[1]), X.columns)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(f'{name}_correlation.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    X.hist()\n",
    "    plt.savefig(f'{name}_histogram.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    y.hist()\n",
    "    plt.savefig(f'{name}_y_histogram.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "# save_figs(X,y,\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e66bd",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab175e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_dt_depth(x_train, x_test, y_train, y_test, outpath):\n",
    "    # function for fitting trees of various depths on the training data using cross-validation\n",
    "    # Referenced via https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6\n",
    "    def run_cross_validation_on_trees(X, y, tree_depths, cv=5, scoring='accuracy'):\n",
    "        cv_scores_list = []\n",
    "        cv_scores_std = []\n",
    "        cv_scores_mean = []\n",
    "        accuracy_scores = []\n",
    "        for depth in tree_depths:\n",
    "            tree_model = DecisionTreeClassifier(max_depth=depth)\n",
    "            cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n",
    "            fitted = tree_model.fit(X, y)\n",
    "            cv_scores_list.append(cv_scores)\n",
    "            cv_scores_mean.append(cv_scores.mean())\n",
    "            cv_scores_std.append(cv_scores.std())\n",
    "            accuracy_scores.append(fitted.score(X, y))\n",
    "        cv_scores_mean = np.array(cv_scores_mean)\n",
    "        cv_scores_std = np.array(cv_scores_std)\n",
    "        accuracy_scores = np.array(accuracy_scores)\n",
    "        return cv_scores_mean, cv_scores_std, accuracy_scores\n",
    "\n",
    "    # function for plotting cross-validation results\n",
    "    def plot_cross_validation_on_trees(depths, cv_scores_mean, cv_scores_std, accuracy_scores, title):\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "        ax.plot(depths, cv_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
    "        ax.fill_between(depths, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n",
    "        ylim = plt.ylim(0.45,1.05)\n",
    "        ax.plot(depths, accuracy_scores, '-*', label='train accuracy', alpha=0.9)\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        ax.set_xlabel('Tree depth', fontsize=14)\n",
    "        ax.set_ylabel('Accuracy', fontsize=14)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xticks(depths)\n",
    "        ax.legend()\n",
    "        fig.savefig(outpath, bbox_inches='tight')\n",
    "        fig.clf()\n",
    "        return fig\n",
    "\n",
    "    # fitting trees of depth 1 to 24\n",
    "    sm_tree_depths = range(1,15)\n",
    "    sm_cv_scores_mean, sm_cv_scores_std, sm_accuracy_scores = run_cross_validation_on_trees(x_train, y_train, sm_tree_depths)\n",
    "    \n",
    "    # plotting accuracy\\\n",
    "    fig = plot_cross_validation_on_trees(sm_tree_depths, sm_cv_scores_mean, sm_cv_scores_std, sm_accuracy_scores, \n",
    "                                   'Accuracy per decision tree depth on training data')\n",
    "\n",
    "\n",
    "    # Empty array that will hold our classifiers\n",
    "    classifiers = []\n",
    "    dif = sm_cv_scores_mean - sm_accuracy_scores\n",
    "    return dif\n",
    "\n",
    "def dt(x_train, x_test, y_train, y_test, depth, outpath, name):\n",
    "    clf = DecisionTreeClassifier(max_depth=depth)\n",
    "    # Create the learning curve visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        clf,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    visualizer.fit(x_train, y_train)        # Fit the data to the visualizer\n",
    "#     visualizer.show(outpath=outpath, clear_figure=True)\n",
    "    visualizer.finalize()\n",
    "    # Get access to the axes object and modify labels\n",
    "    plt.savefig(outpath, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    start = time.time()\n",
    "    clf.fit(x_train,y_train)\n",
    "    end = time.time()\n",
    "    print(\"Training DT took\" , (end - start)*1000, \" ms\")\n",
    "    accuracy = clf.score(x_test,y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801801c1",
   "metadata": {},
   "source": [
    "# MLP (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f9c333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nn(x_train, x_test, y_train, y_test, outpath, name, hidden_layers, lr):\n",
    "    clf = MLPClassifier(max_iter=800, hidden_layer_sizes=hidden_layers, learning_rate_init=lr, \n",
    "                        shuffle=False, random_state=12)\n",
    "    # Create the learning curve visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        clf,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    visualizer.fit(x_train, y_train)\n",
    "    visualizer.finalize()\n",
    "    # Get access to the axes object and modify labels\n",
    "    plt.savefig(outpath, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    start = time.time()\n",
    "    clf.fit(x_train,y_train)\n",
    "    end = time.time()\n",
    "    print(\"Training NN took\" , (end - start)*1000, \" ms\")\n",
    "    accuracy = visualizer.score(x_test,y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db804697",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70fc51a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def boosting(x_train, x_test, y_train, y_test, best_dt_depth, n_estimators, outpath, name):\n",
    "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=best_dt_depth), n_estimators=n_estimators)\n",
    "    # Create the learning curve visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        clf\n",
    "    )\n",
    "\n",
    "    visualizer.fit(x_train, y_train)\n",
    "    visualizer.finalize()\n",
    "    # Get access to the axes object and modify labels\n",
    "    plt.savefig(outpath, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    start = time.time()\n",
    "    clf.fit(x_train,y_train)\n",
    "    end = time.time()\n",
    "    print(\"Training AdaBoost took\" , (end - start)*1000, \" ms\")\n",
    "    accuracy = clf.score(x_test,y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522f299",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45fcbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def svm(x_train, x_test, y_train, y_test, outpath):\n",
    "    clf = svm.SVC()\n",
    "    # Create the learning curve visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        clf\n",
    "    )\n",
    "\n",
    "    visualizer.fit(x_train, y_train)\n",
    "    visualizer.finalize()\n",
    "    # Get access to the axes object and modify labels\n",
    "    plt.savefig(outpath, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    start = time.time()\n",
    "    clf.fit(x_train,y_train)\n",
    "    end = time.time()\n",
    "    print(\"Training SVM took\" , (end - start)*1000, \" ms\")\n",
    "    accuracy = clf.score(x_test,y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cfe2c",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8a1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x_train, x_test, y_train, y_test):\n",
    "    clf = KNeighborsClassifier(3)\n",
    "    # Create the learning curve visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        clf\n",
    "    )\n",
    "\n",
    "    visualizer.fit(x_train, y_train)        # Fit the data to the visualizer\n",
    "   # visualizer.show()           # Finalize and render the figure\n",
    "\n",
    "    clf.fit(x_train,y_train)\n",
    "    accuracy = clf.score(x_test,y_test)\n",
    "\n",
    "    \n",
    "    return visualizer, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310336d",
   "metadata": {},
   "source": [
    "# Generate Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a2151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Generating Data for mobile\n",
      "=============================\n",
      "Training DT took 3.2320022583007812  ms\n",
      "DT accuracy: 0.725%\n",
      "Training AdaBoost took 156.4490795135498  ms\n",
      "Boosting accuracy: 0.745%\n"
     ]
    }
   ],
   "source": [
    "# X, y = load_heart()\n",
    "X, y = load_mobile()\n",
    "# X, y = load_weather()\n",
    "# X, y = load_adult()\n",
    "# dt nn boosting svm knn\n",
    "data = [load_mobile(), load_heart()]\n",
    "names = ['mobile', 'heart']\n",
    "for i in range(2):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[i][0],data[i][1], test_size=0.1, random_state=12)\n",
    "    name=names[i]\n",
    "    print('=============================')\n",
    "    print(f\"Generating Data for {name}\")\n",
    "    print('=============================')\n",
    "    dif = get_ideal_dt_depth(x_train, x_test, y_train, y_test, f'out/dt_pruning_{name}')\n",
    "    ideal_depth = np.argsort(dif)[-2]\n",
    "    accuracy = dt(x_train, x_test, y_train, y_test, ideal_depth, f'out/dt_{name}.png',name)\n",
    "    print(\"DT accuracy: {0:.3f}%\".format(accuracy))\n",
    "    \n",
    "#     accuracy = nn(x_train, x_test, y_train, y_test, f'out/nn_{name}.png', name, hidden_layers=(64,12), lr=0.001)\n",
    "#     print(\"NN accuracy: {0:.3f}%\".format(accuracy))\n",
    "#     accuracy = nn(x_train, x_test, y_train, y_test, f'out/nn_big_{name}.png', name, (1000,), 0.003)\n",
    "#     print(\"NN With More Layers accuracy: {0:.3f}%\".format(accuracy))\n",
    "    \n",
    "    accuracy = boosting(x_train, x_test, y_train, y_test, ideal_depth, 50, f'out/boosting_{name}.png', name)\n",
    "    print(\"Boosting accuracy: {0:.3f}%\".format(accuracy))\n",
    "    accuracy = boosting(x_train, x_test, y_train, y_test, ideal_depth, 500, f'out/boosting_more_estimators_{name}.png', name)\n",
    "    print(\"Boosting More Estimators accuracy: {0:.3f}%\".format(accuracy))\n",
    "    \n",
    "    \n",
    "    accuracy = svm(x_train, x_test, y_train, y_test, f'out/boosting_more_estimators_{name}.png')\n",
    "    print(\"SVM accuracy: {0:.3f}%\".format(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01187a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7f74e933fd57653de6749693f631840ec630c35e9a1541559774eff7e03757"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
